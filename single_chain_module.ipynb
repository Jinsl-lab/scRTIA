{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954fde5-f8ac-4233-8e88-99648cfe7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Callable, List, Union, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import awkward as ak\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# 激活函数注册表\n",
    "class ActivationRegistry:\n",
    "    \"\"\"管理不同激活函数的类\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.func_map: Dict[str, Callable] = {}\n",
    "        self.register_defaults()\n",
    "        \n",
    "    def register_defaults(self):\n",
    "        \"\"\"注册默认激活函数\"\"\"\n",
    "        self.register('tanh', nn.Tanh)\n",
    "        self.register('relu', nn.ReLU)\n",
    "        self.register('silu', nn.SiLU)\n",
    "        self.register('mish', nn.Mish)\n",
    "        self.register('sigmoid', nn.Sigmoid)\n",
    "        self.register('softmax', lambda dim=1: nn.Softmax(dim=dim))\n",
    "        self.register('log_softmax', lambda dim=1: nn.LogSoftmax(dim=dim))\n",
    "    \n",
    "    def register(self, name: str, func: Callable):\n",
    "        \"\"\"注册新的激活函数\"\"\"\n",
    "        if name in self.func_map:\n",
    "            logging.info(f'激活函数 \"{name}\" 已注册，将被覆盖。')\n",
    "        self.func_map[name] = func\n",
    "    \n",
    "    def get(self, name: str, **kwargs) -> Callable:\n",
    "        \"\"\"获取注册的激活函数\"\"\"\n",
    "        if name not in self.func_map:\n",
    "            raise KeyError(f'激活函数 \"{name}\" 未注册。')\n",
    "        func = self.func_map[name]\n",
    "        if callable(func):\n",
    "            return func(**kwargs) if kwargs else func()\n",
    "        return func\n",
    "    \n",
    "    def list_registered(self) -> List[str]:\n",
    "        \"\"\"列出所有注册的激活函数\"\"\"\n",
    "        return list(self.func_map.keys())\n",
    "\n",
    "activation_registry = ActivationRegistry()\n",
    "\n",
    "# 单层模块\n",
    "class SingleLayer(nn.Module):\n",
    "    \"\"\"包含激活函数、归一化和dropout的模块\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dim: Union[int, bool] = False,\n",
    "                 norm: Union[str, bool] = False,\n",
    "                 trans: Union[str, bool] = False,\n",
    "                 drop: Union[float, bool] = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        # 添加归一化层\n",
    "        if norm == 'bn':\n",
    "            layers.append(nn.BatchNorm1d(dim))\n",
    "        elif norm == 'ln':\n",
    "            layers.append(nn.LayerNorm(dim))\n",
    "            \n",
    "        # 添加激活函数\n",
    "        if trans:\n",
    "            layers.append(activation_registry.get(trans))\n",
    "            \n",
    "        # 添加dropout层\n",
    "        if drop:\n",
    "            layers.append(nn.Dropout(drop))\n",
    "            \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "# MLP模块\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"多层感知器\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 features: List[int],\n",
    "                 hid_trans: str = 'relu',\n",
    "                 trans: Union[str, bool] = False,\n",
    "                 hid_norm: Union[str, bool] = False,\n",
    "                 norm: Union[str, bool] = False,\n",
    "                 hid_drop: Union[float, bool] = False,\n",
    "                 drop: Union[float, bool] = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 应用全局归一化和dropout（如果指定）\n",
    "        if norm:\n",
    "            hid_norm = out_norm = norm\n",
    "        else:\n",
    "            out_norm = False\n",
    "            \n",
    "        if drop:\n",
    "            hid_drop = out_drop = drop\n",
    "        else:\n",
    "            out_drop = False\n",
    "            \n",
    "        # 构建MLP层\n",
    "        layers = []\n",
    "        for i in range(1, len(features)):\n",
    "            layers.append(nn.Linear(features[i-1], features[i]))\n",
    "            if i < len(features) - 1:  # 隐藏层\n",
    "                layers.append(SingleLayer(features[i], hid_norm, hid_trans, hid_drop))\n",
    "            else:  # 输出层\n",
    "                layers.append(SingleLayer(features[i], out_norm, trans, out_drop))\n",
    "                \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Transformer的位置编码\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, dropout, max_len):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # 创建位置编码矩阵\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        # 应用正弦和余弦函数\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Transformer编码器\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"TCR序列编码器\"\"\"\n",
    "    \n",
    "    def __init__(self, params, hdim, num_seq_labels):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        \n",
    "        # 嵌入层和位置编码\n",
    "        self.embedding = nn.Embedding(num_seq_labels, params['embedding_size'], padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            params['embedding_size'], \n",
    "            params['dropout'], \n",
    "            params['max_tcr_length']\n",
    "        )\n",
    "        \n",
    "        # Transformer编码器层\n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=params['embedding_size'],\n",
    "            nhead=params['num_heads'],\n",
    "            dim_feedforward=params['embedding_size'] * params['forward_expansion'],\n",
    "            dropout=params['dropout']\n",
    "        )\n",
    "        \n",
    "        # Transformer编码器\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layers,\n",
    "            num_layers=params['encoding_layers']\n",
    "        )\n",
    "        \n",
    "        # 全连接层降维\n",
    "        self.fc_reduction = nn.Linear(params['max_tcr_length'] * params['embedding_size'], hdim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 嵌入和位置编码\n",
    "        x = self.embedding(x) * math.sqrt(self.embedding.embedding_dim)\n",
    "        x = x.transpose(0, 1)  # [seq_len, batch, features]\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer处理\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0, 1)  # [batch, seq_len, features]\n",
    "        \n",
    "        # 展平和降维\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc_reduction(x)\n",
    "        return x\n",
    "\n",
    "# Transformer解码器\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"TCR序列解码器\"\"\"\n",
    "    \n",
    "    def __init__(self, params, hdim, num_seq_labels):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.num_seq_labels = num_seq_labels\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 全连接层升维\n",
    "        self.fc_upsample = nn.Linear(hdim, params['max_tcr_length'] * params['embedding_size'])\n",
    "        \n",
    "        # 嵌入层和位置编码\n",
    "        self.embedding = nn.Embedding(num_seq_labels, params['embedding_size'], padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            params['embedding_size'], \n",
    "            params['dropout'], \n",
    "            params['max_tcr_length']\n",
    "        )\n",
    "        \n",
    "        # Transformer解码器层\n",
    "        decoder_layers = nn.TransformerDecoderLayer(\n",
    "            d_model=params['embedding_size'],\n",
    "            nhead=params['num_heads'],\n",
    "            dim_feedforward=params['embedding_size'] * params['forward_expansion'],\n",
    "            dropout=params['dropout']\n",
    "        )\n",
    "        \n",
    "        # Transformer解码器\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layers,\n",
    "            num_layers=params['decoding_layers']\n",
    "        )\n",
    "        \n",
    "        # 输出层\n",
    "        self.fc_out = nn.Linear(params['embedding_size'], num_seq_labels)\n",
    "        \n",
    "    def forward(self, hidden_state, target_sequence):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 升维\n",
    "        hidden_state = self.fc_upsample(hidden_state)\n",
    "        \n",
    "        # 调整形状\n",
    "        shape = (hidden_state.shape[0], self.params['max_tcr_length'], self.params['embedding_size'])\n",
    "        hidden_state = hidden_state.view(shape).transpose(0, 1)\n",
    "        \n",
    "        # 准备目标序列\n",
    "        target_sequence = target_sequence[:, :-1].transpose(0, 1)  # 去掉EOS\n",
    "        target_sequence = self.embedding(target_sequence) * math.sqrt(self.embedding.embedding_dim)\n",
    "        target_sequence = self.positional_encoding(target_sequence)\n",
    "        \n",
    "        # 生成掩码\n",
    "        seq_len = target_sequence.shape[0]\n",
    "        target_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(self.device)\n",
    "        \n",
    "        # Transformer处理\n",
    "        x = self.transformer_decoder(target_sequence, hidden_state, tgt_mask=target_mask)\n",
    "        \n",
    "        # 调整形状并输出\n",
    "        x = self.fc_out(x).transpose(0, 1)\n",
    "        return x\n",
    "\n",
    "# V/J基因分类器\n",
    "class TransformerClassifier(nn.Module):\n",
    "    \"\"\"V/J基因分类器\"\"\"\n",
    "    \n",
    "    def __init__(self, params, hdim, v_dim, j_dim):\n",
    "        super().__init__()\n",
    "        self.v_classifier = MLP([hdim, 128, v_dim])\n",
    "        self.j_classifier = MLP([hdim, 128, j_dim])\n",
    "    \n",
    "    def forward(self, latent):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        v_pred = self.v_classifier(latent)\n",
    "        j_pred = self.j_classifier(latent)\n",
    "        return v_pred, j_pred\n",
    "\n",
    "# Transformer自动编码器（带分类）\n",
    "class TransformerAutoencoderWithClassification(nn.Module):\n",
    "    \"\"\"带V/J基因分类的Transformer自动编码器\"\"\"\n",
    "    \n",
    "    def __init__(self, params, num_seq_labels, v_dim, j_dim):\n",
    "        super().__init__()\n",
    "        hdim = params['embedding_size']\n",
    "        \n",
    "        # 编码器、解码器和分类器\n",
    "        self.encoder = TransformerEncoder(params, hdim, num_seq_labels)\n",
    "        self.decoder = TransformerDecoder(params, hdim, num_seq_labels)\n",
    "        self.classifier = TransformerClassifier(params, hdim, v_dim, j_dim)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        latent = self.encoder(src)\n",
    "        reconstruction = self.decoder(latent, tgt)\n",
    "        v_pred, j_pred = self.classifier(latent)\n",
    "        return reconstruction, latent, v_pred, j_pred\n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"编码序列\"\"\"\n",
    "        return self.encoder(src)\n",
    "    \n",
    "    def classify(self, latent):\n",
    "        \"\"\"分类V/J基因\"\"\"\n",
    "        return self.classifier(latent)\n",
    "    \n",
    "    def decode(self, latent, tgt=None, max_length=30):\n",
    "        \"\"\"解码序列\"\"\"\n",
    "        self.eval()\n",
    "        if tgt is None:\n",
    "            return self._autoregressive_decode(latent, max_length)\n",
    "        else:\n",
    "            return self.decoder(latent, tgt)\n",
    "    \n",
    "    def _autoregressive_decode(self, latent, max_length):\n",
    "        \"\"\"自回归解码\"\"\"\n",
    "        batch_size = latent.size(0)\n",
    "        device = latent.device\n",
    "        \n",
    "        # 初始化生成序列\n",
    "        generated = torch.full((batch_size, max_length), 0, dtype=torch.long, device=device)\n",
    "        generated[:, 0] = 1  # 起始标记\n",
    "        \n",
    "        # 逐步生成\n",
    "        for i in range(1, max_length):\n",
    "            output = self.decoder(latent, generated[:, :i])\n",
    "            next_token = output[:, -1:].argmax(-1)\n",
    "            generated[:, i] = next_token.squeeze()\n",
    "            \n",
    "        return generated\n",
    "\n",
    "# TCR数据集（带V/J基因）\n",
    "class CDR3DatasetWithVJ(Dataset):\n",
    "    \"\"\"包含V/J基因的TCR数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, v_genes, j_genes, token_to_idx, max_seq_len, \n",
    "                 v_gene_to_idx, j_gene_to_idx):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            sequences: CDR3序列列表\n",
    "            v_genes: V基因标签列表\n",
    "            j_genes: J基因标签列表\n",
    "            token_to_idx: CDR3词汇映射\n",
    "            max_seq_len: 最大序列长度\n",
    "            v_gene_to_idx: V基因到索引的映射\n",
    "            j_gene_to_idx: J基因到索引的映射\n",
    "        \"\"\"\n",
    "        self.sequences = sequences\n",
    "        self.v_genes = v_genes\n",
    "        self.j_genes = j_genes\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.v_gene_to_idx = v_gene_to_idx\n",
    "        self.j_gene_to_idx = j_gene_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"获取单个样本\"\"\"\n",
    "        seq = self.sequences[idx]\n",
    "        v_gene = self.v_genes[idx]\n",
    "        j_gene = self.j_genes[idx]\n",
    "        \n",
    "        # 编码CDR3序列\n",
    "        encoded = [self.token_to_idx['<SOS>']]  # 起始标记\n",
    "        for aa in seq:\n",
    "            encoded.append(self.token_to_idx.get(aa, self.token_to_idx['<PAD>']))  # 处理未知字符\n",
    "        encoded.append(self.token_to_idx['<EOS>'])  # 结束标记\n",
    "        \n",
    "        # 填充或截断序列\n",
    "        if len(encoded) < self.max_seq_len:\n",
    "            encoded += [self.token_to_idx['<PAD>']] * (self.max_seq_len - len(encoded))\n",
    "        else:\n",
    "            encoded = encoded[:self.max_seq_len]\n",
    "        \n",
    "        # 转换为tensor\n",
    "        seq_tensor = torch.tensor(encoded, dtype=torch.long)\n",
    "        \n",
    "        # 处理V/J基因标签\n",
    "        v_label = self.v_gene_to_idx.get(v_gene, 0)  # 0表示未知\n",
    "        j_label = self.j_gene_to_idx.get(j_gene, 0)\n",
    "        \n",
    "        return seq_tensor, torch.tensor(v_label, dtype=torch.long), torch.tensor(j_label, dtype=torch.long)\n",
    "\n",
    "# 提前停止类\n",
    "class EarlyStopping:\n",
    "    \"\"\"用于提前停止训练的工具类\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: 在停止前等待的epoch数\n",
    "            min_delta: 作为改善的最小变化量\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "# 准备TCR β链数据\n",
    "def prepare_tcr_beta_data(data):\n",
    "    \"\"\"\n",
    "    从MuData对象中提取TCR β链数据（CDR3序列、V和J基因）\n",
    "    \n",
    "    参数:\n",
    "        data: MuData对象，包含airr模态中的TCR数据\n",
    "        \n",
    "    返回:\n",
    "        tuple: (all_sequences, all_v_genes, all_j_genes, token_to_idx, max_seq_len, v_gene_to_idx, j_gene_to_idx, v_dim, j_dim)\n",
    "    \"\"\"\n",
    "    all_sequences = []\n",
    "    all_v_genes = []\n",
    "    all_j_genes = []\n",
    "    \n",
    "    # 提取所有V/J基因的唯一标签\n",
    "    v_genes_set = set()\n",
    "    j_genes_set = set()\n",
    "    \n",
    "    # 遍历每个细胞的TCR链\n",
    "    for chains in data['airr'].obsm['airr']:\n",
    "        for chain in chains:\n",
    "            if hasattr(chain, 'locus') and chain.locus == 'TRB':\n",
    "                if hasattr(chain, 'cdr3') and chain.cdr3:\n",
    "                    seq = chain.cdr3_aa.strip().upper()\n",
    "                    if seq:\n",
    "                        # 提取V基因和J基因\n",
    "                        v_gene = chain.v_call.split('*')[0] if hasattr(chain, 'v_call') else \"UNK\"\n",
    "                        j_gene = chain.j_call.split('*')[0] if hasattr(chain, 'j_call') else \"UNK\"\n",
    "                        \n",
    "                        all_sequences.append(seq)\n",
    "                        all_v_genes.append(v_gene)\n",
    "                        all_j_genes.append(j_gene)\n",
    "                        \n",
    "                        v_genes_set.add(v_gene)\n",
    "                        j_genes_set.add(j_gene)\n",
    "    \n",
    "    # 创建词汇表\n",
    "    vocab = set()\n",
    "    for seq in all_sequences:\n",
    "        vocab.update(seq)\n",
    "    vocab = ['<PAD>', '<SOS>', '<EOS>'] + sorted(vocab)\n",
    "    token_to_idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "    \n",
    "    # 添加未知标记\n",
    "    v_genes_list = sorted(v_genes_set)\n",
    "    v_gene_to_idx = {gene: idx+1 for idx, gene in enumerate(v_genes_list)}\n",
    "    v_gene_to_idx[\"UNK\"] = 0\n",
    "    v_dim = len(v_gene_to_idx)\n",
    "    \n",
    "    j_genes_list = sorted(j_genes_set)\n",
    "    j_gene_to_idx = {gene: idx+1 for idx, gene in enumerate(j_genes_list)}\n",
    "    j_gene_to_idx[\"UNK\"] = 0\n",
    "    j_dim = len(j_gene_to_idx)\n",
    "    \n",
    "    # 计算最大序列长度\n",
    "    max_seq_len = max(len(seq) for seq in all_sequences) + 2  # +2 for SOS and EOS\n",
    "    \n",
    "    return (all_sequences, all_v_genes, all_j_genes, token_to_idx, max_seq_len, \n",
    "            v_gene_to_idx, j_gene_to_idx, v_dim, j_dim)\n",
    "\n",
    "# 训练周期函数\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, \n",
    "               token_to_idx, seq_criterion, vj_criterion):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_recon_loss = 0.0\n",
    "    total_v_loss = 0.0\n",
    "    total_j_loss = 0.0\n",
    "    \n",
    "    for sequences, v_labels, j_labels in dataloader:\n",
    "        sequences = sequences.to(device)\n",
    "        v_labels = v_labels.to(device)\n",
    "        j_labels = j_labels.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        reconstruction, latent, v_pred, j_pred = model(sequences, sequences)\n",
    "        \n",
    "        # 计算序列重建损失 - 修复形状不匹配问题\n",
    "        # 原始序列去掉第一个token (SOS) 作为目标\n",
    "        targets = sequences[:, 1:]\n",
    "        \n",
    "        # 重建输出去掉最后一个token\n",
    "        outputs = reconstruction[:, :-1, :]\n",
    "        \n",
    "        # 确保输出和目标序列长度匹配\n",
    "        seq_len = min(outputs.size(1), targets.size(1))\n",
    "        outputs = outputs[:, :seq_len, :]\n",
    "        targets = targets[:, :seq_len]\n",
    "        \n",
    "        # 展平张量以计算损失\n",
    "        outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "        targets_flat = targets.contiguous().view(-1)\n",
    "        \n",
    "        recon_loss = seq_criterion(outputs_flat, targets_flat)\n",
    "        \n",
    "        # 计算V/J基因分类损失\n",
    "        v_loss = vj_criterion(v_pred, v_labels)\n",
    "        j_loss = vj_criterion(j_pred, j_labels)\n",
    "        \n",
    "        # 组合总损失\n",
    "        loss = 1* recon_loss +  1* v_loss + 1*  j_loss\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 记录各项损失\n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_v_loss += v_loss.item()\n",
    "        total_j_loss += j_loss.item()\n",
    "    \n",
    "    # 计算平均损失\n",
    "    num_batches = len(dataloader)\n",
    "    return (\n",
    "        total_loss / num_batches,\n",
    "        total_recon_loss / num_batches,\n",
    "        total_v_loss / num_batches,\n",
    "        total_j_loss / num_batches\n",
    "    )\n",
    "\n",
    "# 验证周期函数\n",
    "def validate_epoch(model, dataloader, device, token_to_idx, seq_criterion, vj_criterion):\n",
    "    \"\"\"验证一个epoch\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_recon_loss = 0.0\n",
    "    total_v_loss = 0.0\n",
    "    total_j_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sequences, v_labels, j_labels in dataloader:\n",
    "            sequences = sequences.to(device)\n",
    "            v_labels = v_labels.to(device)\n",
    "            j_labels = j_labels.to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            reconstruction, latent, v_pred, j_pred = model(sequences, sequences)\n",
    "            \n",
    "            # 计算序列重建损失 - 修复形状不匹配问题\n",
    "            # 原始序列去掉第一个token (SOS) 作为目标\n",
    "            targets = sequences[:, 1:]\n",
    "            \n",
    "            # 重建输出去掉最后一个token\n",
    "            outputs = reconstruction[:, :-1, :]\n",
    "            \n",
    "            # 确保输出和目标序列长度匹配\n",
    "            seq_len = min(outputs.size(1), targets.size(1))\n",
    "            outputs = outputs[:, :seq_len, :]\n",
    "            targets = targets[:, :seq_len]\n",
    "            \n",
    "            # 展平张量以计算损失\n",
    "            outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n",
    "            targets_flat = targets.contiguous().view(-1)\n",
    "            \n",
    "            recon_loss = seq_criterion(outputs_flat, targets_flat)\n",
    "            \n",
    "            # 计算V/J基因分类损失\n",
    "            v_loss = vj_criterion(v_pred, v_labels)\n",
    "            j_loss = vj_criterion(j_pred, j_labels)\n",
    "            \n",
    "            # 组合总损失\n",
    "            loss = 1* recon_loss + 1*  v_loss + 1* j_loss\n",
    "            \n",
    "            # 记录损失\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_v_loss += v_loss.item()\n",
    "            total_j_loss += j_loss.item()\n",
    "    \n",
    "    # 计算平均损失\n",
    "    num_batches = len(dataloader)\n",
    "    return (\n",
    "        total_loss / num_batches,\n",
    "        total_recon_loss / num_batches,\n",
    "        total_v_loss / num_batches,\n",
    "        total_j_loss / num_batches\n",
    "    )\n",
    "\n",
    "# 主训练函数\n",
    "def main_training(data, output_dir=\"tcr_model\"):\n",
    "    \"\"\"主训练函数\"\"\"\n",
    "    # 准备设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    \n",
    "    # 准备TCR数据\n",
    "    results = prepare_tcr_beta_data(data)\n",
    "    (all_sequences, all_v_genes, all_j_genes, token_to_idx, \n",
    "     max_seq_len, v_gene_to_idx, j_gene_to_idx, v_dim, j_dim) = results\n",
    "    \n",
    "    print(f\"加载了{len(all_sequences)}个TCR β链序列\")\n",
    "    print(f\"最大序列长度: {max_seq_len}\")\n",
    "    print(f\"词汇表大小: {len(token_to_idx)}\")\n",
    "    print(f\"V基因类别数: {v_dim}\")\n",
    "    print(f\"J基因类别数: {j_dim}\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = CDR3DatasetWithVJ(\n",
    "        all_sequences, all_v_genes, all_j_genes, token_to_idx, max_seq_len, \n",
    "        v_gene_to_idx, j_gene_to_idx\n",
    "    )\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    batch_size = 64\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    # 模型配置\n",
    "    params = {\n",
    "        'embedding_size': 32,\n",
    "        'num_heads': 4,\n",
    "        'forward_expansion': 4,\n",
    "        'encoding_layers': 2,\n",
    "        'decoding_layers': 2,\n",
    "        'dropout': 0.1,\n",
    "        'max_tcr_length': max_seq_len\n",
    "    }\n",
    "    \n",
    "    # 初始化模型\n",
    "    vocab_size = len(token_to_idx)\n",
    "    model = TransformerAutoencoderWithClassification(params, vocab_size, v_dim, j_dim)\n",
    "    model = model.to(device)\n",
    "    print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # 定义损失函数\n",
    "    seq_criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx['<PAD>'])\n",
    "    vj_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 定义优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 设置早停\n",
    "    early_stopper = EarlyStopping(patience=10, min_delta=0.001)\n",
    "    \n",
    "    # 训练循环\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # 保存训练历史\n",
    "    train_history = {'loss': [], 'recon_loss': [], 'v_loss': [], 'j_loss': []}\n",
    "    val_history = {'loss': [], 'recon_loss': [], 'v_loss': [], 'j_loss': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练\n",
    "        train_loss, train_recon, train_v, train_j = train_epoch(\n",
    "            model, train_loader, seq_criterion, optimizer, device, \n",
    "            token_to_idx, seq_criterion, vj_criterion\n",
    "        )\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_recon, val_v, val_j = validate_epoch(\n",
    "            model, val_loader, device, token_to_idx, seq_criterion, vj_criterion\n",
    "        )\n",
    "        \n",
    "        # 记录历史\n",
    "        train_history['loss'].append(train_loss)\n",
    "        train_history['recon_loss'].append(train_recon)\n",
    "        train_history['v_loss'].append(train_v)\n",
    "        train_history['j_loss'].append(train_j)\n",
    "        \n",
    "        val_history['loss'].append(val_loss)\n",
    "        val_history['recon_loss'].append(val_recon)\n",
    "        val_history['v_loss'].append(val_v)\n",
    "        val_history['j_loss'].append(val_j)\n",
    "        \n",
    "        # 打印进度\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  训练损失: 总损失={train_loss:.4f}, 重建={train_recon:.4f}, V={train_v:.4f}, J={train_j:.4f}\")\n",
    "        print(f\"  验证损失: 总损失={val_loss:.4f}, 重建={val_recon:.4f}, V={val_v:.4f}, J={val_j:.4f}\")\n",
    "        \n",
    "        # 检查是否应该保存模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'params': params,\n",
    "                'token_to_idx': token_to_idx,\n",
    "                'v_gene_to_idx': v_gene_to_idx,\n",
    "                'j_gene_to_idx': j_gene_to_idx,\n",
    "                'max_seq_len': max_seq_len,\n",
    "                'v_dim': v_dim,\n",
    "                'j_dim': j_dim,\n",
    "                'train_history': train_history,\n",
    "                'val_history': val_history,\n",
    "                'total_loss': val_loss,\n",
    "            }, f\"{output_dir}/best_model.pth\")\n",
    "            print(f\"    保存最佳模型, 验证损失: {val_loss:.4f}\")\n",
    "        \n",
    "        # 检查是否应该提前停止\n",
    "        if early_stopper(val_loss):\n",
    "            print(f\"早停触发于epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"训练完成！\")\n",
    "    \n",
    "    # 返回模型和相关信息\n",
    "    return model, token_to_idx, v_gene_to_idx, j_gene_to_idx, max_seq_len, v_dim, j_dim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mosaicenv]",
   "language": "python",
   "name": "conda-env-mosaicenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
