{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8ad4b-c2a1-4e2a-9b81-1456202b726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设备检测函数\n",
    "def get_device():\n",
    "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 数据预处理函数 - 修复：从gex模态获取数据\n",
    "def preprocess_data(data):\n",
    "    \"\"\"拼接X_alpha和X_beta并进行归一化\"\"\"\n",
    "    # 从gex模态提取alpha和beta特征\n",
    "    gex_data = data['gex']\n",
    "    X_alpha = gex_data.obsm['X_alpha']\n",
    "    X_beta = gex_data.obsm['X_beta']\n",
    "    \n",
    "    # 检查形状是否匹配\n",
    "    if X_alpha.shape[0] != X_beta.shape[0]:\n",
    "        raise ValueError(\"X_alpha and X_beta have different number of samples\")\n",
    "    \n",
    "    # 拼接特征\n",
    "    X_combined = np.concatenate([X_alpha, X_beta], axis=1)\n",
    "    \n",
    "    # 分别归一化alpha和beta部分\n",
    "    scaler_alpha = StandardScaler()\n",
    "    scaler_beta = StandardScaler()\n",
    "    \n",
    "    alpha_dim = X_alpha.shape[1]\n",
    "    beta_dim = X_beta.shape[1]\n",
    "    X_alpha_norm = scaler_alpha.fit_transform(X_alpha)\n",
    "    X_beta_norm = scaler_beta.fit_transform(X_beta)\n",
    "    \n",
    "    # 重新组合归一化后的数据\n",
    "    X_norm = np.concatenate([X_alpha_norm, X_beta_norm], axis=1)\n",
    "    \n",
    "    return X_norm, alpha_dim, beta_dim, scaler_alpha, scaler_beta\n",
    "\n",
    "# GMM组件\n",
    "class GMMComponents(nn.Module):\n",
    "    def __init__(self, latent_dim, n_centroids):\n",
    "        super().__init__()\n",
    "        self.n_centroids = n_centroids\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # 初始化GMM参数\n",
    "        self.mu_c = nn.Parameter(torch.randn(latent_dim, n_centroids))\n",
    "        self.logvar_c = nn.Parameter(torch.zeros(latent_dim, n_centroids))  # 使用logvar更稳定\n",
    "        self.pi = nn.Parameter(torch.ones(n_centroids) / n_centroids)  # 初始等概率\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"计算后验概率gamma和GMM参数\"\"\"\n",
    "        n_centroids = self.n_centroids\n",
    "        N = z.size(0)\n",
    "        \n",
    "        # 计算log p(c)\n",
    "        log_pi = torch.log(self.pi.clamp(min=1e-10))\n",
    "        \n",
    "        # 扩展z的维度用于批量计算\n",
    "        z_expanded = z.unsqueeze(2).expand(-1, -1, n_centroids)\n",
    "        \n",
    "        # 计算log p(z|c)\n",
    "        logvar_c = self.logvar_c.clamp(min=-10, max=10)  # 防止数值溢出\n",
    "        var_c = torch.exp(logvar_c)\n",
    "        \n",
    "        # 高斯概率密度函数\n",
    "        log_p_z_c = -0.5 * (\n",
    "            (z_expanded - self.mu_c.unsqueeze(0)) ** 2 / var_c.unsqueeze(0)\n",
    "            + logvar_c.unsqueeze(0)\n",
    "            + math.log(2 * math.pi)\n",
    "        ).sum(dim=1)\n",
    "        \n",
    "        # 计算联合概率log p(z,c) = log p(c) + log p(z|c)\n",
    "        log_joint = log_pi.unsqueeze(0) + log_p_z_c\n",
    "        \n",
    "        # 计算后验概率gamma\n",
    "        gamma = torch.exp(log_joint - torch.logsumexp(log_joint, dim=1, keepdim=True))\n",
    "        \n",
    "        return gamma, self.mu_c, var_c, self.pi\n",
    "    \n",
    "    def init_gmm_params(self, z_data):\n",
    "        \"\"\"使用EM算法初始化GMM参数\"\"\"\n",
    "        print(\"Initializing GMM parameters with EM algorithm...\")\n",
    "        gmm = GaussianMixture(n_components=self.n_centroids, covariance_type='diag')\n",
    "        gmm.fit(z_data)\n",
    "        \n",
    "        # 更新参数\n",
    "        self.mu_c.data.copy_(torch.tensor(gmm.means_.T, dtype=torch.float32))\n",
    "        self.logvar_c.data.copy_(torch.log(torch.tensor(gmm.covariances_.T.clip(1e-3), dtype=torch.float32)))\n",
    "        self.pi.data.copy_(torch.tensor(gmm.weights_, dtype=torch.float32))\n",
    "        print(\"GMM initialization complete!\")\n",
    "\n",
    "# GMM的KL散度损失\n",
    "def gmm_loss(z, gamma, z_mean, z_logvar, gmm_mu, gmm_var, gmm_pi):\n",
    "    \"\"\"计算编码器分布与GMM先验之间的KL散度\"\"\"\n",
    "    # 1. 编码器分布：N(z_mean, exp(z_logvar))\n",
    "    # 2. GMM先验：sum_c pi_c * N(gmm_mu_c, gmm_var_c)\n",
    "    \n",
    "    n_centroids = gmm_pi.size(0)\n",
    "    eps = 1e-8\n",
    "    \n",
    "    # 计算每个数据点在每个聚类中心下的概率密度\n",
    "    z_expanded = z.unsqueeze(2).expand(-1, -1, n_centroids)\n",
    "    log_p_z_given_c = -0.5 * (\n",
    "        (z_expanded - gmm_mu.unsqueeze(0)) ** 2 / (gmm_var.unsqueeze(0) + eps)\n",
    "        + torch.log(2 * math.pi * gmm_var.unsqueeze(0) + eps)\n",
    "    ).sum(dim=1)\n",
    "    \n",
    "    # 计算混合概率\n",
    "    log_p_z = torch.logsumexp(torch.log(gmm_pi + eps) + log_p_z_given_c, dim=1)\n",
    "    \n",
    "    # 计算编码器分布的熵\n",
    "    entropy = 0.5 * (1 + z_logvar + math.log(2 * math.pi))\n",
    "    \n",
    "    # 计算KL散度：KL(q(z|x) || p(z)) = -熵 - log p(z)\n",
    "    kld = -entropy.sum(dim=1) - log_p_z\n",
    "    \n",
    "    return kld.mean()\n",
    "\n",
    "# 构建多层感知机\n",
    "def build_mlp(layers, dropout_rate=0.1):\n",
    "    modules = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        modules.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        modules.append(nn.BatchNorm1d(layers[i + 1]))\n",
    "        modules.append(nn.ReLU())\n",
    "        if dropout_rate > 0:\n",
    "            modules.append(nn.Dropout(dropout_rate))\n",
    "    return nn.Sequential(*modules)\n",
    "\n",
    "# 编码器\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 构建编码器网络\n",
    "        layers = [input_dim] + hidden_dims\n",
    "        self.net = build_mlp(layers)\n",
    "        \n",
    "        # 潜在空间参数\n",
    "        self.fc_mean = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + eps * std\n",
    "        return mean\n",
    "\n",
    "# 解码器（双输出结构）\n",
    "class DualDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, alpha_dim, beta_dim):\n",
    "        super().__init__()\n",
    "        self.alpha_dim = alpha_dim\n",
    "        self.beta_dim = beta_dim\n",
    "        \n",
    "        # 共享的隐藏层\n",
    "        layers = [latent_dim] + hidden_dims\n",
    "        self.shared_net = build_mlp(layers[:-1])\n",
    "        \n",
    "        # 特定模态的输出层\n",
    "        self.fc_alpha = nn.Linear(hidden_dims[-2], alpha_dim)\n",
    "        self.fc_beta = nn.Linear(hidden_dims[-2], beta_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        h = self.shared_net(z)\n",
    "        recon_alpha = self.fc_alpha(h)\n",
    "        recon_beta = self.fc_beta(h)\n",
    "        return recon_alpha, recon_beta\n",
    "\n",
    "# GMM-VAE模型\n",
    "class GMMVAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, n_centroids, alpha_dim, beta_dim,\n",
    "                 encoder_hidden=[256, 128], decoder_hidden=[128, 256]):\n",
    "        super().__init__()\n",
    "        self.alpha_dim = alpha_dim\n",
    "        self.beta_dim = beta_dim\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = Encoder(input_dim, encoder_hidden, latent_dim)\n",
    "        \n",
    "        # 解码器（双输出）\n",
    "        self.decoder = DualDecoder(latent_dim, decoder_hidden, alpha_dim, beta_dim)\n",
    "        \n",
    "        # GMM组件\n",
    "        self.gmm = GMMComponents(latent_dim, n_centroids)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 编码\n",
    "        z_mean, z_logvar = self.encoder(x)\n",
    "        z = self.encoder.reparameterize(z_mean, z_logvar)\n",
    "        \n",
    "        # GMM后验\n",
    "        gamma, gmm_mu, gmm_var, gmm_pi = self.gmm(z)\n",
    "        \n",
    "        # 解码（分别重构alpha和beta）\n",
    "        recon_alpha, recon_beta = self.decoder(z)\n",
    "        \n",
    "        return {\n",
    "            'z': z,\n",
    "            'z_mean': z_mean,\n",
    "            'z_logvar': z_logvar,\n",
    "            'gamma': gamma,\n",
    "            'gmm_mu': gmm_mu,\n",
    "            'gmm_var': gmm_var,\n",
    "            'gmm_pi': gmm_pi,\n",
    "            'recon_alpha': recon_alpha,\n",
    "            'recon_beta': recon_beta\n",
    "        }\n",
    "    \n",
    "    def init_gmm(self, dataloader):\n",
    "        \"\"\"使用预训练编码器初始化GMM参数\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            all_z = []\n",
    "            for batch in dataloader:\n",
    "                # 修复此处 - 批次是包含一个元素的元组\n",
    "                x = batch[0].to(device)  # 获取元组的第一个元素\n",
    "                z_mean, _ = self.encoder(x)\n",
    "                z = self.encoder.reparameterize(z_mean, torch.zeros_like(z_mean))\n",
    "                all_z.append(z.cpu())\n",
    "            \n",
    "            all_z = torch.cat(all_z, dim=0).numpy()\n",
    "            self.gmm.init_gmm_params(all_z)\n",
    "        \n",
    "        self.train()\n",
    "\n",
    "# 训练函数 - 添加模态权重参数\n",
    "def train_gmm_vae(model, data, epochs=50, gmm_epochs=10, batch_size=512, lr=1e-3,\n",
    "                  alpha_weight=0.5, beta_weight=0.5):\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # 准备数据\n",
    "    X_combined, alpha_dim, beta_dim, _, _ = preprocess_data(data)\n",
    "    X_tensor = torch.tensor(X_combined, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 第一阶段：预训练标准VAE\n",
    "    print(\"Phase 1: Pre-training standard VAE\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        recon_loss_total = 0\n",
    "        kld_loss_total = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            # 注意：批次是一个包含单个张量的元组\n",
    "            x = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(x)\n",
    "            recon_alpha = outputs['recon_alpha']\n",
    "            recon_beta = outputs['recon_beta']\n",
    "            z_mean = outputs['z_mean']\n",
    "            z_logvar = outputs['z_logvar']\n",
    "            \n",
    "            # 分割输入数据\n",
    "            x_alpha = x[:, :alpha_dim]\n",
    "            x_beta = x[:, alpha_dim:]\n",
    "            \n",
    "            # 计算重构损失 (MSE) - 分别计算两个模态\n",
    "            recon_loss_alpha = F.mse_loss(recon_alpha, x_alpha, reduction='sum') / x.size(0)\n",
    "            recon_loss_beta = F.mse_loss(recon_beta, x_beta, reduction='sum') / x.size(0)\n",
    "            \n",
    "            # 加权重构损失\n",
    "            recon_loss = alpha_weight * recon_loss_alpha + beta_weight * recon_loss_beta\n",
    "            \n",
    "            # 计算标准KL损失 (单位高斯先验)\n",
    "            kl_div = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "            kl_div = kl_div / x.size(0)\n",
    "            \n",
    "            loss = recon_loss + kl_div * 0.01\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            recon_loss_total += recon_loss.item()\n",
    "            kld_loss_total += kl_div.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f'[Pre-train] Epoch {epoch+1}/{epochs} | '\n",
    "              f'Loss: {avg_loss:.4f} | Recon: {recon_loss_total/len(loader):.4f} | '\n",
    "              f'KLD: {kld_loss_total/len(loader):.4f}')\n",
    "    \n",
    "    # 第二阶段：初始化GMM\n",
    "    print(\"Phase 2: Initializing GMM parameters\")\n",
    "    model.init_gmm(loader)\n",
    "    \n",
    "    # 第三阶段：联合训练整个模型\n",
    "    print(\"Phase 3: Training GMM-VAE\")\n",
    "    for epoch in range(gmm_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        recon_loss_total = 0\n",
    "        kld_loss_total = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            # 注意：批次是一个包含单个张量的元组\n",
    "            x = batch[0].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(x)\n",
    "            recon_alpha = outputs['recon_alpha']\n",
    "            recon_beta = outputs['recon_beta']\n",
    "            z = outputs['z']\n",
    "            z_mean = outputs['z_mean']\n",
    "            z_logvar = outputs['z_logvar']\n",
    "            \n",
    "            # 分割输入数据\n",
    "            x_alpha = x[:, :alpha_dim]\n",
    "            x_beta = x[:, alpha_dim:]\n",
    "            \n",
    "            # 计算重构损失 - 分别计算两个模态\n",
    "            recon_loss_alpha = F.mse_loss(recon_alpha, x_alpha, reduction='sum') / x.size(0)\n",
    "            recon_loss_beta = F.mse_loss(recon_beta, x_beta, reduction='sum') / x.size(0)\n",
    "            \n",
    "            # 加权重构损失\n",
    "            recon_loss = alpha_weight * recon_loss_alpha + beta_weight * recon_loss_beta\n",
    "            \n",
    "            # 计算GMM KL损失\n",
    "            kl_div = gmm_loss(z, outputs['gamma'], z_mean, z_logvar,\n",
    "                              outputs['gmm_mu'], outputs['gmm_var'], outputs['gmm_pi'])\n",
    "            \n",
    "            # 总损失\n",
    "            loss = recon_loss + kl_div\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            recon_loss_total += recon_loss.item()\n",
    "            kld_loss_total += kl_div.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(loader)\n",
    "        print(f'[GMM Train] Epoch {epoch+1}/{gmm_epochs} | '\n",
    "              f'Loss: {avg_loss:.4f} | Recon: {recon_loss_total/len(loader):.4f} | '\n",
    "              f'GMM KLD: {kld_loss_total/len(loader):.4f}')\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mosaicenv]",
   "language": "python",
   "name": "conda-env-mosaicenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
